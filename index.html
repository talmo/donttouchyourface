<!DOCTYPE html>
<html>
<head>
    <title>Don't touch your face!</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style type="text/css">
        body {
          font-family: Arial;
          background-color: #eeeeee;
      }
      h1 {
        color: red;
    }
    #info {
        font-size: 10px;
        color: #222222;
    }
</style>
</head>
<body>
    <div id="page" style="width: 100%;">
        <div id="main" style="display: table; margin-left: auto; margin-right: auto;">
            <video id="video" playsinline style="display: none;"></video>
            <canvas id="output"></canvas>
            <div id="danger-zone" style="display: none; margin-left: auto; margin-right: auto;"><h1>Danger zone!!!</h1></div>
            <div id="info" style="display: table; margin-left: auto; margin-right: auto;"></div>
        </div>
    </div>
</body>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>

<script type="text/javascript">
    let videoWidth = 600;
    let videoHeight = 480;
    // const color = 'aqua';
    let color = 'aqua';
    const boundingBoxColor = 'red';
    const lineWidth = 2;


    function isAndroid() {
      return /Android/i.test(navigator.userAgent);
  }

  function isiOS() {
      return /iPhone|iPad|iPod/i.test(navigator.userAgent);
  }

    // export function isMobile() {
        function isMobile() {
          return isAndroid() || isiOS();
      }

      async function setupCamera() {
          if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            throw new Error(
                'Browser API navigator.mediaDevices.getUserMedia not available');
        }

        const mobile = isMobile();
        if (mobile) {
            videoWidth = 320;
            videoHeight = 600;
        }

        const video = document.getElementById('video');
        video.width = videoWidth;
        video.height = videoHeight;

        const stream = await navigator.mediaDevices.getUserMedia({
            'audio': false,
            'video': {
              facingMode: 'user',
              width: mobile ? undefined : videoWidth,
              height: mobile ? undefined : videoHeight,
          },
      });
        video.srcObject = stream;

        return new Promise((resolve) => {
            video.onloadedmetadata = () => {
              resolve(video);
          };
      });
    }

    async function loadVideo() {
      const video = await setupCamera();
      video.play();

      return video;
  }


  function toTuple({y, x}) {
      return [y, x];
  }

// export function drawPoint(ctx, y, x, r, color) {
    function drawPoint(ctx, y, x, r, color) {
      ctx.beginPath();
      ctx.arc(x, y, r, 0, 2 * Math.PI);
      ctx.fillStyle = color;
      ctx.fill();
  }

/**
 * Draws a line on a canvas, i.e. a joint
 */
// export function drawSegment([ay, ax], [by, bx], color, scale, ctx) {
    function drawSegment([ay, ax], [by, bx], color, scale, ctx) {
      ctx.beginPath();
      ctx.moveTo(ax * scale, ay * scale);
      ctx.lineTo(bx * scale, by * scale);
      ctx.lineWidth = lineWidth;
      ctx.strokeStyle = color;
      ctx.stroke();
  }


/**
 * Draws a pose skeleton by looking up all adjacent keypoints/joints
 */
// export function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {
    function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {
      const adjacentKeyPoints =
      posenet.getAdjacentKeyPoints(keypoints, minConfidence);

      adjacentKeyPoints.forEach((keypoints) => {
        drawSegment(
            toTuple(keypoints[0].position), toTuple(keypoints[1].position), color,
            scale, ctx);
    });
  }

/**
 * Draw pose keypoints onto a canvas
 */
// export function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {
    function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {
      for (let i = 0; i < keypoints.length; i++) {
        const keypoint = keypoints[i];

        if (keypoint.score < minConfidence) {
          continue;
      }

      const {y, x} = keypoint.position;
      drawPoint(ctx, y * scale, x * scale, 3, color);
  }
}

/**
 * Draw the bounding box of a pose. For example, for a whole person standing
 * in an image, the bounding box will begin at the nose and extend to one of
 * ankles
 */
// export function drawBoundingBox(keypoints, ctx) {
    function drawBoundingBox(keypoints, ctx) {
      const boundingBox = posenet.getBoundingBox(keypoints);

      ctx.rect(
          boundingBox.minX, boundingBox.minY, boundingBox.maxX - boundingBox.minX,
          boundingBox.maxY - boundingBox.minY);

      ctx.strokeStyle = boundingBoxColor;
      ctx.stroke();
  }

/**
 * Converts an arary of pixel data into an ImageData object
 */
// export async function renderToCanvas(a, ctx) {
    async function renderToCanvas(a, ctx) {
      const [height, width] = a.shape;
      const imageData = new ImageData(width, height);

      const data = await a.data();

      for (let i = 0; i < height * width; ++i) {
        const j = i * 4;
        const k = i * 3;

        imageData.data[j + 0] = data[k + 0];
        imageData.data[j + 1] = data[k + 1];
        imageData.data[j + 2] = data[k + 2];
        imageData.data[j + 3] = 255;
    }

    ctx.putImageData(imageData, 0, 0);
}

/**
 * Draw an image on a canvas
 */
// export function renderImageToCanvas(image, size, canvas) {
    function renderImageToCanvas(image, size, canvas) {
      canvas.width = size[0];
      canvas.height = size[1];
      const ctx = canvas.getContext('2d');

      ctx.drawImage(image, 0, 0);
  }


/**
 * Draw heatmap values, one of the model outputs, on to the canvas
 * Read our blog post for a description of PoseNet's heatmap outputs
 * https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5
 */
// export function drawHeatMapValues(heatMapValues, outputStride, canvas) {
    function drawHeatMapValues(heatMapValues, outputStride, canvas) {
      const ctx = canvas.getContext('2d');
      const radius = 5;
      const scaledValues = heatMapValues.mul(tf.scalar(outputStride, 'int32'));

      drawPoints(ctx, scaledValues, radius, color);
  }

/**
 * Used by the drawHeatMapValues method to draw heatmap points on to
 * the canvas
 */
 function drawPoints(ctx, points, radius, color) {
  const data = points.buffer().values;

  for (let i = 0; i < data.length; i += 2) {
    const pointY = data[i];
    const pointX = data[i + 1];

    if (pointX !== 0 && pointY !== 0) {
      ctx.beginPath();
      ctx.arc(pointX, pointY, radius, 0, 2 * Math.PI);
      ctx.fillStyle = color;
      ctx.fill();
  }
}
}

/**
 * Draw offset vector values, one of the model outputs, on to the canvas
 * Read our blog post for a description of PoseNet's offset vector outputs
 * https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5
 */
// export function drawOffsetVectors(
function drawOffsetVectors(
    heatMapValues, offsets, outputStride, scale = 1, ctx) {
  const offsetPoints =
  posenet.singlePose.getOffsetPoints(heatMapValues, outputStride, offsets);

  const heatmapData = heatMapValues.buffer().values;
  const offsetPointsData = offsetPoints.buffer().values;

  for (let i = 0; i < heatmapData.length; i += 2) {
    const heatmapY = heatmapData[i] * outputStride;
    const heatmapX = heatmapData[i + 1] * outputStride;
    const offsetPointY = offsetPointsData[i];
    const offsetPointX = offsetPointsData[i + 1];

    drawSegment(
        [heatmapY, heatmapX], [offsetPointY, offsetPointX], color, scale, ctx);
}
}



function detectPoseInRealTime(video, net) {
  const canvas = document.getElementById('output');
  const ctx = canvas.getContext('2d');

  // since images are being fed from a webcam, we want to feed in the
  // original image and then just flip the keypoints' x coordinates. If instead
  // we flip the image, then correcting left-right keypoint pairs requires a
  // permutation on all the keypoints.
  const flipPoseHorizontal = true;

  canvas.width = videoWidth;
  canvas.height = videoHeight;

  async function poseDetectionFrame() {
    // if (guiState.changeToArchitecture) {
    //   // Important to purge variables and free up GPU memory
    //   guiState.net.dispose();
    //   toggleLoadingUI(true);
    //   guiState.net = await posenet.load({
    //     architecture: guiState.changeToArchitecture,
    //     outputStride: guiState.outputStride,
    //     inputResolution: guiState.inputResolution,
    //     multiplier: guiState.multiplier,
    //   });
    //   toggleLoadingUI(false);
    //   guiState.architecture = guiState.changeToArchitecture;
    //   guiState.changeToArchitecture = null;
    // }

    // if (guiState.changeToMultiplier) {
    //   guiState.net.dispose();
    //   toggleLoadingUI(true);
    //   guiState.net = await posenet.load({
    //     architecture: guiState.architecture,
    //     outputStride: guiState.outputStride,
    //     inputResolution: guiState.inputResolution,
    //     multiplier: +guiState.changeToMultiplier,
    //     quantBytes: guiState.quantBytes
    //   });
    //   toggleLoadingUI(false);
    //   guiState.multiplier = +guiState.changeToMultiplier;
    //   guiState.changeToMultiplier = null;
    // }

    // if (guiState.changeToOutputStride) {
    //   // Important to purge variables and free up GPU memory
    //   guiState.net.dispose();
    //   toggleLoadingUI(true);
    //   guiState.net = await posenet.load({
    //     architecture: guiState.architecture,
    //     outputStride: +guiState.changeToOutputStride,
    //     inputResolution: guiState.inputResolution,
    //     multiplier: guiState.multiplier,
    //     quantBytes: guiState.quantBytes
    //   });
    //   toggleLoadingUI(false);
    //   guiState.outputStride = +guiState.changeToOutputStride;
    //   guiState.changeToOutputStride = null;
    // }

    // if (guiState.changeToInputResolution) {
    //   // Important to purge variables and free up GPU memory
    //   guiState.net.dispose();
    //   toggleLoadingUI(true);
    //   guiState.net = await posenet.load({
    //     architecture: guiState.architecture,
    //     outputStride: guiState.outputStride,
    //     inputResolution: +guiState.changeToInputResolution,
    //     multiplier: guiState.multiplier,
    //     quantBytes: guiState.quantBytes
    //   });
    //   toggleLoadingUI(false);
    //   guiState.inputResolution = +guiState.changeToInputResolution;
    //   guiState.changeToInputResolution = null;
    // }

    // if (guiState.changeToQuantBytes) {
    //   // Important to purge variables and free up GPU memory
    //   guiState.net.dispose();
    //   toggleLoadingUI(true);
    //   guiState.net = await posenet.load({
    //     architecture: guiState.architecture,
    //     outputStride: guiState.outputStride,
    //     inputResolution: guiState.inputResolution,
    //     multiplier: guiState.multiplier,
    //     quantBytes: guiState.changeToQuantBytes
    //   });
    //   toggleLoadingUI(false);
    //   guiState.quantBytes = guiState.changeToQuantBytes;
    //   guiState.changeToQuantBytes = null;
    // }

    // Begin monitoring code for frames per second
    // stats.begin();

    let poses = [];
    let minPoseConfidence = 0.1;
    let minPartConfidence = 0.1;
    // switch (guiState.algorithm) {
    //   case 'single-pose':
    // const pose = await guiState.net.estimatePoses(video, {
        const pose = await net.estimatePoses(video, {
          flipHorizontal: flipPoseHorizontal,
          decodingMethod: 'single-person'
      });
        poses = poses.concat(pose);
    // minPoseConfidence = +guiState.singlePoseDetection.minPoseConfidence;
    // minPartConfidence = +guiState.singlePoseDetection.minPartConfidence;
        // break;
    //   case 'multi-pose':
    //     let all_poses = await guiState.net.estimatePoses(video, {
    //       flipHorizontal: flipPoseHorizontal,
    //       decodingMethod: 'multi-person',
    //       maxDetections: guiState.multiPoseDetection.maxPoseDetections,
    //       scoreThreshold: guiState.multiPoseDetection.minPartConfidence,
    //       nmsRadius: guiState.multiPoseDetection.nmsRadius
    //     });

    //     poses = poses.concat(all_poses);
    //     minPoseConfidence = +guiState.multiPoseDetection.minPoseConfidence;
    //     minPartConfidence = +guiState.multiPoseDetection.minPartConfidence;
    //     break;
    // }

    ctx.clearRect(0, 0, videoWidth, videoHeight);

    // if (guiState.output.showVideo) {
      ctx.save();
      ctx.scale(-1, 1);
      ctx.translate(-videoWidth, 0);
      ctx.drawImage(video, 0, 0, videoWidth, videoHeight);
      ctx.restore();
    // }

    // For each pose (i.e. person) detected in an image, loop through the poses
    // and draw the resulting skeleton and keypoints if over certain confidence
    // scores
    let info_div = document.getElementById('info');
    // info_div.textContent = '';
    info_div.innerHTML = '';
    info_div.style.display = 'block';

    let dangerzone_div = document.getElementById('danger-zone');

    poses.forEach(({score, keypoints}) => {
      if (score >= minPoseConfidence) {
        // console.log(keypoints)
        // if (guiState.output.showPoints) {

          // 0 nose
          // 1 leftEye
          // 2 rightEye
          // 3 leftEar
          // 4 rightEar
          // 9 leftWrist
          // 10 rightWrist
          let min_dist = 10000;
          info_div.textContent = '';
          for (const wrist of [keypoints[9], keypoints[10]]) {
            for (const face of [keypoints[0], keypoints[1], keypoints[2]]) {
                if (wrist.score > minPartConfidence && face.score > minPartConfidence) {
                    let dist = Math.pow(Math.pow(wrist["position"]["x"] - face["position"]["x"], 2) + Math.pow(wrist["position"]["y"] - face["position"]["y"], 2), 0.5);
                    info_div.innerHTML += "Dist ("+ wrist["part"] +" <-> "+ face["part"] +"): " + dist + "\n<br>";
                    min_dist = Math.min(min_dist, dist);
                }

            }
        }

        if (min_dist < 175) {
            // info_div.style.border = "5px solid red";
            // video.style.border = "5px solid red";
            canvas.style.border = "5px solid red";
            color = "red";
            dangerzone_div.style.display = "table";

        } else {
            // info_div.style.border = "5px solid green";
            // video.style.border = "5px solid green";
            canvas.style.border = "3px solid green";
            color = "aqua";
            dangerzone_div.style.display = "none";
        }
        drawKeypoints(keypoints, minPartConfidence, ctx);


        // }
        // if (guiState.output.showSkeleton) {
        //   drawSkeleton(keypoints, minPartConfidence, ctx);
        // }
        // if (guiState.output.showBoundingBox) {
        //   drawBoundingBox(keypoints, ctx);
        // }
    }
});

    // End monitoring code for frames per second
    // stats.end();

    requestAnimationFrame(poseDetectionFrame);
}

poseDetectionFrame();
}


// export async function bindPage() {
    async function bindPage() {
  // toggleLoadingUI(true);
  // const net = await posenet.load({
  //   architecture: guiState.input.architecture,
  //   outputStride: guiState.input.outputStride,
  //   inputResolution: guiState.input.inputResolution,
  //   multiplier: guiState.input.multiplier,
  //   quantBytes: guiState.input.quantBytes
  // });
  // toggleLoadingUI(false);
  
  if isMobile() {
    const net = await posenet.load({
      architecture: 'MobileNetV1',
      outputStride: 16,
      inputResolution: { width: 640, height: 480 },
      multiplier: 0.75
  });
} else {
  const net = await posenet.load({
      architecture: 'ResNet50',
      outputStride: 32,
      inputResolution: { width: 257, height: 200 },
      quantBytes: 2
  });
}

let video;

try {
    video = await loadVideo();
} catch (e) {
    let info = document.getElementById('info');
    info.textContent = 'this browser does not support video capture,' +
    'or this device does not have a camera';
    info.style.display = 'block';
    throw e;
}

detectPoseInRealTime(video, net);
}


bindPage();

</script>
</html>
